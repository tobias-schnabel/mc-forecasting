{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T21:07:13.718495Z",
     "start_time": "2024-09-11T21:07:13.706415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import getpass\n",
    "from pylatex import Table, NoEscape, Command\n",
    "\n",
    "export_username = \"ts\"  # Only save tables to dropbox on my machine\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_comparison_table(dataframes, estimator_names, metrics=['RMSE', 'MAE', 'rMAE'], decimal_places=3):\n",
    "    def calc_stats(df, metrics):\n",
    "        stats = {}\n",
    "        for metric in metrics:\n",
    "            stats[f'{metric}_min'] = df[metric].min()\n",
    "            stats[f'{metric}_mean'] = df[metric].mean()\n",
    "            stats[f'{metric}_median'] = df[metric].median()\n",
    "            stats[f'{metric}_max'] = df[metric].max()\n",
    "        \n",
    "        time_metrics = ['fit_time', 'predict_time', 'optimize_time']\n",
    "        for time_metric in time_metrics:\n",
    "            stats[f'{time_metric}_mean'] = df[time_metric].mean()\n",
    "        \n",
    "        return pd.Series(stats)\n",
    "\n",
    "    all_stats = {}\n",
    "    for df, name in zip(dataframes, estimator_names):\n",
    "        all_stats[name] = calc_stats(df, metrics)\n",
    "\n",
    "    comparison_table = pd.DataFrame(all_stats).T\n",
    "\n",
    "    # Create multi-level columns\n",
    "    column_tuples = [(metric, stat) for metric in metrics for stat in ['min', 'mean', 'median', 'max']] + \\\n",
    "                    [(metric, 'mean') for metric in ['fit time', 'predict time', 'optimize time']]\n",
    "    comparison_table.columns = pd.MultiIndex.from_tuples(column_tuples)\n",
    "\n",
    "    # Reorder columns\n",
    "    new_order = metrics + ['fit time', 'predict time', 'optimize time']\n",
    "    comparison_table = comparison_table.reindex(columns=new_order, level=0)\n",
    "\n",
    "    return comparison_table.round(decimal_places)\n",
    "\n",
    "\n",
    "def create_latex_table_content(df, caption, label):\n",
    "    table_content = []\n",
    "    table_content.append(r'\\begin{table}[H]')\n",
    "    table_content.append(r'\\centering')\n",
    "    table_content.append(f'\\\\caption{{{caption}}}')\n",
    "    table_content.append(f'\\\\label{{{label}}}')\n",
    "    table_content.append(r'\\resizebox{\\textwidth}{!}{')\n",
    "    \n",
    "    num_cols = len(df.columns) + 1\n",
    "    table_format = '|l|' + 'c' * (num_cols - 1) + '|'\n",
    "    table_content.append(f'\\\\begin{{tabular}}{{{table_format}}}')\n",
    "    \n",
    "    table_content.append(r'\\hline')\n",
    "    # Create main headers\n",
    "    main_metrics = df.columns.get_level_values(0).unique()\n",
    "    main_headers = ['Estimator'] + [f'\\\\multicolumn{{4}}{{c|}}{{{metric}}}' if metric in ['RMSE', 'MAE', 'rMAE'] else metric for metric in main_metrics]\n",
    "    table_content.append(' & '.join(main_headers) + r' \\\\')\n",
    "    \n",
    "    # Add horizontal line between metric names and subcolumn names\n",
    "    table_content.append(r'\\cline{2-' + str(num_cols) + '}')\n",
    "    \n",
    "    # Create sub-headers\n",
    "    sub_headers = ['']\n",
    "    for metric in main_metrics:\n",
    "        if metric in ['RMSE', 'MAE', 'rMAE']:\n",
    "            sub_headers.extend(['min', 'mean', 'median', 'max'])\n",
    "        else:\n",
    "            sub_headers.append('mean')\n",
    "    table_content.append(' & '.join(sub_headers) + r' \\\\')\n",
    "    \n",
    "    table_content.append(r'\\hline')\n",
    "    \n",
    "    # Find minimum values for each column\n",
    "    min_values = df.min()\n",
    "\n",
    "    for i, (index, row) in enumerate(df.iterrows()):\n",
    "        row_content = [f\"{index}\"]\n",
    "        for (col, subcol), value in row.items():\n",
    "            if pd.isna(value):\n",
    "                cell_content = ''\n",
    "            elif isinstance(value, (int, float)):\n",
    "                if value == 0:\n",
    "                    cell_content = '0'\n",
    "                elif value.is_integer():\n",
    "                    cell_content = f\"\\\\num{{{int(value):,}}}\"\n",
    "                elif value < 1 and value > 0:\n",
    "                    cell_content = f\"\\\\num{{{value:.3f}}}\"\n",
    "                else:\n",
    "                    cell_content = f\"\\\\num{{{value:.3f}}}\"\n",
    "                \n",
    "                # Highlight minimum value\n",
    "                if value == min_values[(col, subcol)] and subcol != 'max' and not df[col][subcol].eq(value).all():\n",
    "                    cell_content = f\"\\\\cellcolor{{gray!25}}\\\\textbf{{{cell_content}}}\"\n",
    "            else:\n",
    "                cell_content = f\"{value}\"\n",
    "            row_content.append(cell_content)\n",
    "        table_content.append(' & '.join(row_content) + r' \\\\')\n",
    "        \n",
    "        # Add horizontal line after each estimator\n",
    "        table_content.append(r'\\hline')\n",
    "    \n",
    "    table_content.append(r'\\end{tabular}')\n",
    "    table_content.append(r'}')\n",
    "    table_content.append(r'\\end{table}')\n",
    "    \n",
    "    return '\\n'.join(table_content)\n",
    "\n",
    "\n",
    "\n",
    "def save_latex_table_content(content, filename):\n",
    "    \"\"\"\n",
    "    Save LaTeX table content to a file, but only if on the specified machine.\n",
    "    \n",
    "    Args:\n",
    "    content (str): LaTeX table content to save\n",
    "    filename (str): Name of the file to save (without extension)\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    username = getpass.getuser()\n",
    "    if username == export_username:\n",
    "        filepath = \"/Users/ts/Library/CloudStorage/Dropbox/Apps/Overleaf/Dissertation Oxford/Tables\"\n",
    "        full_filename = os.path.join(filepath, filename + \".tex\")\n",
    "        \n",
    "        with open(full_filename, 'w') as file:\n",
    "            file.write(content)\n",
    "        \n",
    "        print(f\"Table content saved to {full_filename}\")\n",
    "    else:\n",
    "        print(\"Table content not saved (not on the specified machine)\")"
   ],
   "id": "61993d23d01a70fa",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparison of 64 and 32 bit float precision for MC-NNM 56",
   "id": "768d2eb8c2e40bd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T21:07:14.491017Z",
     "start_time": "2024-09-11T21:07:14.484817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MCNNM_56_64 = pd.read_parquet('../results/MCNNM_56_results.parquet')\n",
    "MCNNM_56_32 = pd.read_parquet('../results/MCNNM-56_32_results.parquet')\n",
    "MCNNM_56_32.shape, MCNNM_56_64.shape"
   ],
   "id": "8da6122bf9318471",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2006, 11), (363, 11))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T21:07:14.761933Z",
     "start_time": "2024-09-11T21:07:14.758103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# subset the first 363 rows of the 32 bit precision results (this is where I stopped the training for 64 bit)\n",
    "MCNNM_56_32_trunc = MCNNM_56_32.iloc[:363]\n",
    "MCNNM_56_32_trunc.shape, MCNNM_56_64.shape"
   ],
   "id": "a92d70a195959a4d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((363, 11), (363, 11))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T21:07:15.048762Z",
     "start_time": "2024-09-11T21:07:15.040212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataframes_32_64 = [MCNNM_56_32_trunc, MCNNM_56_64]\n",
    "estimator_names_32_64 = ['32-bit', '64-bit']\n",
    "table_32_64 = create_comparison_table(dataframes_32_64, estimator_names_32_64)\n",
    "print(table_32_64)"
   ],
   "id": "7ef22986b53dfe4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          RMSE                             MAE                           rMAE  \\\n",
      "           min    mean  median      max    min    mean  median     max    min   \n",
      "32-bit  14.245  33.065  30.275  105.003  8.242  18.739  17.650  45.308  0.447   \n",
      "64-bit  13.322  33.033  30.578  105.003  7.547  18.721  17.585  46.387  0.435   \n",
      "\n",
      "                            fit time predict time optimize time  \n",
      "         mean median    max     mean         mean          mean  \n",
      "32-bit  1.750  1.545  4.957      0.0        5.031         2.245  \n",
      "64-bit  1.754  1.565  4.957      0.0        5.041        15.646  \n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that the 64-bit precision model has slightly better performance than the 32-bit model, with lower RMSE and MAE values. The difference is relatively small, but it is consistent across all metrics. The prediction time is also slightly longer for the 64-bit model, which severely compounds during the optimization phase. This is expected, as the 64-bit model has twice the memory requirements and will be slower to compute.",
   "id": "6d8750fc8cadc786"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T21:07:17.059472Z",
     "start_time": "2024-09-11T21:07:17.047178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create your comparison table\n",
    "dataframes = [MCNNM_56_32_trunc, MCNNM_56_64]\n",
    "estimator_names = ['32-bit', '64-bit']\n",
    "table_32_64 = create_comparison_table(dataframes, estimator_names, decimal_places=3)\n",
    "\n",
    "# Generate the LaTeX content\n",
    "latex_32_64 = create_latex_table_content(\n",
    "    table_32_64,\n",
    "    caption=\"Comparison of 32-bit and 64-bit MC-NNM models\",\n",
    "    label=\"tab:mcnnm-comparison\"\n",
    ")\n",
    "\n",
    "# Save the LaTeX content\n",
    "save_latex_table_content(latex_32_64, \"mcnnm_32_64_comparison\")\n"
   ],
   "id": "d0a19fd873a03a59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table content saved to /Users/ts/Library/CloudStorage/Dropbox/Apps/Overleaf/Dissertation Oxford/Tables/mcnnm_32_64_comparison.tex\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9336c473a43018e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
